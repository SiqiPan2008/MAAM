* 373 ultrawide-field pseudocolor and autofluorescence
* deep cnn (VGG-16)
* RP
* both UWPC and UWAF images, can be mentioned
@article{masumoto2019accuracy,
	title={Accuracy of a deep convolutional neural network in detection of retinitis pigmentosa on ultrawide-field images},
	author={Masumoto, Hiroki and Tabuchi, Hitoshi and Nakakura, Shunsuke and Ohsugi, Hideharu and Enno, Hiroki and Ishitobi, Naofumi and Ohsugi, Eiko and Mitamura, Yoshinori},
	journal={PeerJ},
	volume={7},
	pages={e6900},
	year={2019},
	publisher={PeerJ Inc.}
}

@article{kromer2017approach,
	title={An approach for automated segmentation of retinal layers in peripapillary spectralis SD-OCT images using curve regularisation},
	author={Kromer, R and Rahman, S and Filev, F and Klemm, M},
	journal={Insights in Ophthalmology},
	volume={1},
	number={7},
	pages={1--6},
	year={2017}
}

- Important for fundus photography!!!
* Overview
@article{li2021applications,
	title={Applications of deep learning in fundus images: A review},
	author={Li, Tao and Bo, Wang and Hu, Chunyu and Kang, Hong and Liu, Hanruo and Wang, Kai and Fu, Huazhu},
	journal={Medical Image Analysis},
	volume={69},
	pages={101971},
	year={2021},
	publisher={Elsevier}
}

- Important!!!
* Overview, a good resource for elaborating background regarding AI
@article{daich2023artificial,
	title={Artificial intelligence in retinal disease: clinical application, challenges, and future directions},
	author={Daich Varela, Malena and Sen, Sagnik and De Guimaraes, Thales Antonio Cabral and Kabiri, Nathaniel and Pontikos, Nikolas and Balaskas, Konstantinos and Michaelides, Michel},
	journal={Graefe's Archive for Clinical and Experimental Ophthalmology},
	pages={1--15},
	year={2023},
	publisher={Springer}
}

— the Most Common Inherited Retinal Degeneration
* 1670 color fundus photographs
* Inception V3, Inception Resnet V2, and Xception
* early detection of RP
@article{chen2021artificial,
	title={Artificial intelligence--assisted early detection of retinitis pigmentosa—the most common inherited retinal degeneration},
	author={Chen, Ta-Ching and Lim, Wee Shin and Wang, Victoria Y and Ko, Mei-Lan and Chiu, Shu-I and Huang, Yu-Shu and Lai, Feipei and Yang, Chung-May and Hu, Fung-Rong and Jang, Jyh-Shing Roger and others},
	journal={Journal of Digital Imaging},
	volume={34},
	pages={948--958},
	year={2021},
	publisher={Springer}
}

- medium cited
* Lesion-Aware, Important!
@article{fang2019attention,
	title={Attention to lesion: Lesion-aware convolutional neural network for retinal optical coherence tomography image classification},
	author={Fang, Leyuan and Wang, Chong and Li, Shutao and Rabbani, Hossein and Chen, Xiangdong and Liu, Zhimin},
	journal={IEEE transactions on medical imaging},
	volume={38},
	number={8},
	pages={1959--1970},
	year={2019},
	publisher={IEEE}
}

- big cite number
* Fundus Photography
* 75137 publicly available fundus images
@article{gargeya2017automated,
	title={Automated identification of diabetic retinopathy using deep learning},
	author={Gargeya, Rishab and Leng, Theodore},
	journal={Ophthalmology},
	volume={124},
	number={7},
	pages={962--969},
	year={2017},
	publisher={Elsevier}
}

@article{chiu2010automatic,
	title={Automatic segmentation of seven retinal layers in SDOCT images congruent with expert manual segmentation},
	author={Chiu, Stephanie J and Li, Xiao T and Nicholas, Peter and Toth, Cynthia A and Izatt, Joseph A and Farsiu, Sina},
	journal={Optics express},
	volume={18},
	number={18},
	pages={19413--19428},
	year={2010},
	publisher={Optica Publishing Group}
}

- bit cite number
- CTO and fundus images, need to peruse this paper
@article{de2018clinically,
	title={Clinically applicable deep learning for diagnosis and referral in retinal disease},
	author={De Fauw, Jeffrey and Ledsam, Joseph R and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O’Donoghue, Brendan and Visentin, Daniel and others},
	journal={Nature medicine},
	volume={24},
	number={9},
	pages={1342--1350},
	year={2018},
	publisher={Nature Publishing Group}
}

* CNN
* segmentation of preserved photoreceptors in choroideremia and retinitis pigmentosa
@article{camino2018deep,
	title={Deep learning for the segmentation of preserved photoreceptors on en face optical coherence tomography in two inherited retinal diseases},
	author={Camino, Acner and Wang, Zhuo and Wang, Jie and Pennesi, Mark E and Yang, Paul and Huang, David and Li, Dengwang and Jia, Yali},
	journal={Biomedical optics express},
	volume={9},
	number={7},
	pages={3092--3105},
	year={2018},
	publisher={Optica Publishing Group}
}

- a lot of OCT images and medium cited
* OCT
* ResNet, occlusion testing
* CNV, DME, DRUSEN, NORMAL
@article{li2019deep,
	title={Deep learning-based automated detection of retinal diseases using optical coherence tomography images},
	author={Li, Feng and Chen, Hua and Liu, Zheng and Zhang, Xue-dian and Jiang, Min-shan and Wu, Zhi-zheng and Zhou, Kai-qian},
	journal={Biomedical optics express},
	volume={10},
	number={12},
	pages={6204--6226},
	year={2019},
	publisher={Optica Publishing Group}
}

* 64914 images
* fundus
* 12 major fundus diseases
@article{li2022development,
	title={Development and evaluation of a deep learning model for the detection of multiple fundus diseases based on colour fundus photography},
	author={Li, Bing and Chen, Huan and Zhang, Bilei and Yuan, Mingzhen and Jin, Xuemin and Lei, Bo and Xu, Jie and Gu, Wei and Wong, David Chuen Soong and He, Xixi and others},
	journal={British Journal of Ophthalmology},
	volume={106},
	number={8},
	pages={1079--1086},
	year={2022},
	publisher={BMJ Publishing Group Ltd}
}

- big cite number
@article{gulshan2016development,
	title={Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs},
	author={Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and others},
	journal={jama},
	volume={316},
	number={22},
	pages={2402--2410},
	year={2016},
	publisher={American Medical Association}
}

* best corrected visual acuity from OCT
* 3D -> array of 2D
@article{kawczynski2020development,
	title={Development of deep learning models to predict best-corrected visual acuity from optical coherence tomography},
	author={Kawczynski, Michael G and Bengtsson, Thomas and Dai, Jian and Hopkins, J Jill and Gao, Simon S and Willis, Jeffrey R},
	journal={Translational vision science \& technology},
	volume={9},
	number={2},
	pages={51--51},
	year={2020},
	publisher={The Association for Research in Vision and Ophthalmology}
}

@article{mitamura2012diagnostic,
	title={Diagnostic imaging in patients with retinitis pigmentosa},
	author={Mitamura, Yoshinori and Mitamura-Aizawa, Sayaka and Nagasawa, Toshihiko and Katome, Takashi and Eguchi, Hiroshi and Naito, Takeshi},
	journal={The Journal of Medical Investigation},
	volume={59},
	number={1, 2},
	pages={1--11},
	year={2012},
	publisher={The University of Tokushima Faculty of Medicine}
}

* few-shot dataset
* GAN, inception-v3
* rare diseases
@article{yoo2021feasibility,
	title={Feasibility study to improve deep learning in OCT diagnosis of rare retinal diseases with few-shot classification},
	author={Yoo, Tae Keun and Choi, Joon Yul and Kim, Hong Kyu},
	journal={Medical \& Biological Engineering \& Computing},
	volume={59},
	pages={401--415},
	year={2021},
	publisher={Springer}
}

* OCT
* AMD, DME, normal (* macular hole detection)
* flatten, then SVM
@article{srinivasan2014fully,
	title={Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images},
	author={Srinivasan, Pratul P and Kim, Leo A and Mettu, Priyatham S and Cousins, Scott W and Comer, Grant M and Izatt, Joseph A and Farsiu, Sina},
	journal={Biomedical optics express},
	volume={5},
	number={10},
	pages={3568--3577},
	year={2014},
	publisher={Optica Publishing Group}
}

- big cite number
* transfer learning
@article{kermany2018identifying,
	title={Identifying medical diagnoses and treatable diseases by image-based deep learning},
	author={Kermany, Daniel S and Goldbaum, Michael and Cai, Wenjia and Valentim, Carolina CS and Liang, Huiying and Baxter, Sally L and McKeown, Alex and Yang, Ge and Wu, Xiaokang and Yan, Fangbing and others},
	journal={cell},
	volume={172},
	number={5},
	pages={1122--1131},
	year={2018},
	publisher={Elsevier}
}

* 10,770 central fovea cross‑section OCTs
* VGG
* identify key signs
@article{leandro2023oct,
	title={OCT-based deep-learning models for the identification of retinal key signs},
	author={Leandro, Inferrera and Lorenzo, Borsatti and Aleksandar, Miladinovic and Rosa, Giglio and Agostino, Accardo and Daniele, Tognetto},
	journal={Scientific Reports},
	volume={13},
	number={1},
	pages={14628},
	year={2023},
	publisher={Nature Publishing Group UK London}
}

@article{liu2023prediction,
	title={Prediction of visual impairment in retinitis pigmentosa using deep learning and multimodal fundus images},
	author={Liu, Tin Yan Alvin and Ling, Carlthan and Hahn, Leo and Jones, Craig K and Boon, Camiel JF and Singh, Mandeep S},
	journal={British Journal of Ophthalmology},
	volume={107},
	number={10},
	pages={1484--1489},
	year={2023},
	publisher={BMJ Publishing Group Ltd}
}

@inproceedings{fumero2011rim,
	title={RIM-ONE: An open retinal image database for optic nerve evaluation},
	author={Fumero, Francisco and Alay{\'o}n, Silvia and Sanchez, Jos{\'e} L and Sigut, Jose and Gonzalez-Hernandez, M},
	booktitle={2011 24th international symposium on computer-based medical systems (CBMS)},
	pages={1--6},
	year={2011},
	organization={IEEE}
}

@article{smith2016structure,
	title={Structure-function modeling of optical coherence tomography and standard automated perimetry in the retina of patients with autosomal dominant retinitis pigmentosa},
	author={Smith, Travis B and Parker, Maria and Steinkamp, Peter N and Weleber, Richard G and Smith, Ning and Wilson, David J and VPA Clinical Trial Study Group and EZ Working Group},
	journal={PLoS One},
	volume={11},
	number={2},
	pages={e0148022},
	year={2016},
	publisher={Public Library of Science San Francisco, CA USA}
}

@article{bagci2008thickness,
	title={Thickness profiles of retinal layers by optical coherence tomography image segmentation},
	author={Bagci, Ahmet Murat and Shahidi, Mahnaz and Ansari, Rashid and Blair, Michael and Blair, Norman Paul and Zelkha, Ruth},
	journal={American journal of ophthalmology},
	volume={146},
	number={5},
	pages={679--687},
	year={2008},
	publisher={Elsevier}
}

 @book{Ichhpujani_Thakur_2021, address={Singapore}, series={Current Practices in Ophthalmology}, title={Artificial Intelligence and Ophthalmology: Perks, Perils and Pitfalls}, ISBN={9789811606335}, url={https://link.springer.com/10.1007/978-981-16-0634-2}, DOI={10.1007/978-981-16-0634-2}, publisher={Springer Singapore}, year={2021}, collection={Current Practices in Ophthalmology}, language={en} }

 @article{Ronneberger_Fischer_Brox_2015, title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, url={http://arxiv.org/abs/1505.04597}, abstractNote={There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.}, note={arXiv:1505.04597 [cs]}, number={arXiv:1505.04597}, publisher={arXiv}, author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas}, year={2015}, month=may, language={en} }

 @article{Kermany2018, title={Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning}, volume={172}, ISSN={00928674}, DOI={10.1016/j.cell.2018.02.010}, number={5}, journal={Cell}, author={Kermany, Daniel S. and Goldbaum, Michael and Cai, Wenjia and Valentim, Carolina C.S. and Liang, Huiying and Baxter, Sally L. and McKeown, Alex and Yang, Ge and Wu, Xiaokang and Yan, Fangbing and Dong, Justin and Prasadha, Made K. and Pei, Jacqueline and Ting, Magdalene Y.L. and Zhu, Jie and Li, Christina and Hewett, Sierra and Dong, Jason and Ziyar, Ian and Shi, Alexander and Zhang, Runze and Zheng, Lianghong and Hou, Rui and Shi, William and Fu, Xin and Duan, Yaou and Huu, Viet A.N. and Wen, Cindy and Zhang, Edward D. and Zhang, Charlotte L. and Li, Oulan and Wang, Xiaobo and Singer, Michael A. and Sun, Xiaodong and Xu, Jie and Tafreshi, Ali and Lewis, M. Anthony and Xia, Huimin and Zhang, Kang}, year={2018}, month=feb, pages={1122-1131.e9}, language={en} }

 @article{Fang_Wang2019, title={Attention to Lesion: Lesion-Aware Convolutional Neural Network for Retinal Optical Coherence Tomography Image Classification}, volume={38}, ISSN={0278-0062, 1558-254X}, DOI={10.1109/TMI.2019.2898414}, abstractNote={Automatic and accurate classification of retinal optical coherence tomography (OCT) images is essential to assist ophthalmologist in the diagnosis and grading of macular diseases. Clinically, ophthalmologists usually diagnose macular diseases according to the structures of macular lesions, whose morphologies, size, and numbers are important criteria. In this paper, we propose a novel lesion-aware convolutional neural network (LACNN) method for retinal OCT image classification, in which retinal lesions within OCT images are utilized to guide the CNN to achieve more accurate classification. The LACNN simulates the ophthalmologists’ diagnosis that focuses on local lesion-related regions when analyzing the OCT image. Specifically, we firstly design a lesion detection network (LDN) to generate a soft attention map from the whole OCT image. The attention map is then incorporated into a classification network to weight the contributions of local convolutional representations. Guided by the lesion attention map, the classification network can utilize the information from local lesion-related regions to further accelerate the network training process and improve the OCT classification. Our experimental results on two clinically acquired OCT datasets demonstrate the effectiveness and efficiency of the proposed LACNN method for retinal OCT image classification.}, number={8}, journal={IEEE Transactions on Medical Imaging}, author={Fang, Leyuan and Wang, Chong and Li, Shutao and Rabbani, Hossein and Chen, Xiangdong and Liu, Zhimin}, year={2019}, month=aug, pages={1959–1970}, language={en} }

 @article{Son2023, title={An interpretable and interactive deep learning algorithm for a clinically applicable retinal fundus diagnosis system by modelling finding-disease relationship}, volume={13}, ISSN={2045-2322}, DOI={10.1038/s41598-023-32518-3}, abstractNote={Abstract
	The identification of abnormal findings manifested in retinal fundus images and diagnosis of ophthalmic diseases are essential to the management of potentially vision-threatening eye conditions. Recently, deep learning-based computer-aided diagnosis systems (CADs) have demonstrated their potential to reduce reading time and discrepancy amongst readers. However, the obscure reasoning of deep neural networks (DNNs) has been the leading cause to reluctance in its clinical use as CAD systems. Here, we present a novel architectural and algorithmic design of DNNs to comprehensively identify 15 abnormal retinal findings and diagnose 8 major ophthalmic diseases from macula-centered fundus images with the accuracy comparable to experts. We then define a notion of counterfactual attribution ratio (CAR) which luminates the system’s diagnostic reasoning, representing how each abnormal finding contributed to its diagnostic prediction. By using CAR, we show that both quantitative and qualitative interpretation and interactive adjustment of the CAD result can be achieved. A comparison of the model’s CAR with experts’ finding-disease diagnosis correlation confirms that the proposed model identifies the relationship between findings and diseases similarly as ophthalmologists do.}, number={1}, journal={Scientific Reports}, author={Son, Jaemin and Shin, Joo Young and Kong, Seo Taek and Park, Jeonghyuk and Kwon, Gitaek and Kim, Hoon Dong and Park, Kyu Hyung and Jung, Kyu-Hwan and Park, Sang Jun}, year={2023}, month=apr, pages={5934}, language={en} }

 @article{Xu2021, title={Automated diagnoses of age-related macular degeneration and polypoidal choroidal vasculopathy using bi-modal deep convolutional neural networks}, volume={105}, ISSN={0007-1161, 1468-2079}, DOI={10.1136/bjophthalmol-2020-315817}, abstractNote={Methods  A retrospective cross-­sectional study was proposed of patients with AMD or PCV who came to Peking Union Medical College Hospital. Diagnoses of all patients were confirmed by two retinal experts based on diagnostic gold standard for AMD and PCV. Patients with concurrent retinal vascular diseases were excluded. Colour fundus images and spectral domain OCT images were taken from dilated eyes of patients and healthy controls, and anonymised. All images were pre-l­abelled into normal, dry or wet AMD or PCV. ResNet-50 models were used as the backbone and alternate machine learning models including random forest classifiers were constructed for further comparison. For human-­machine comparison, the same testing data set was diagnosed by three retinal experts independently. All images from the same participant were presented only within a single partition subset.
	Results  On a test set of 143 fundus and OCT image pairs from 80 eyes (20 eyes per-­group), the bi-m­ odal DCNN demonstrated the best performance, with accuracy 87.4%, sensitivity 88.8% and specificity 95.6%, and a perfect agreement with diagnostic gold standard (Cohen’s κ 0.828), exceeds slightly over the best expert (Human1, Cohen’s κ 0.810). For recognising PCV, the model outperformed the best expert as well.
	Conclusion  A bi-­modal DCNN for automated classification of AMD and PCV is accurate and promising in the realm of public health.}, number={4}, journal={British Journal of Ophthalmology}, author={Xu, Zhiyan and Wang, Weisen and Yang, Jingyuan and Zhao, Jianchun and Ding, Dayong and He, Feng and Chen, Di and Yang, Zhikun and Li, Xirong and Yu, Weihong and Chen, Youxin}, year={2021}, month=apr, pages={561–566}, language={en} }
	
 @inbook{Andrearczyk2018, address={Cham}, series={Lecture Notes in Computer Science}, title={Deep Multimodal Classification of Image Types in Biomedical Journal Figures}, volume={11018}, ISBN={978-3-319-98931-0}, url={http://link.springer.com/10.1007/978-3-319-98932-7_1}, DOI={10.1007/978-3-319-98932-7_1}, abstractNote={This paper presents a robust method for the classiﬁcation of medical image types in ﬁgures of the biomedical literature using the fusion of visual and textual information. A deep convolutional network is trained to discriminate among 31 image classes including compound ﬁgures, diagnostic image types and generic illustrations, while another shallow convolutional network is used for the analysis of the captions paired with the images. Various fusion methods are analyzed as well as data augmentation approaches. The proposed system is validated on the ImageCLEF 2013 and 2016 ﬁgure and subﬁgure classiﬁcation tasks, largely improving the currently best performance from 83.5% to 93.7% accuracy and 88.4% to 89.0% respectively.}, booktitle={Experimental IR Meets Multilinguality, Multimodality, and Interaction}, publisher={Springer International Publishing}, author={Andrearczyk, Vincent and Müller, Henning}, editor={Bellot, Patrice and Trabelsi, Chiraz and Mothe, Josiane and Murtagh, Fionn and Nie, Jian Yun and Soulier, Laure and SanJuan, Eric and Cappellato, Linda and Ferro, Nicola}, year={2018}, pages={3–14}, collection={Lecture Notes in Computer Science}, language={en} }

 @book{Wolf_Kirchhof_Reim_2006, address={Stuttgart}, title={The ocular fundus: from findings to diagnosis}, ISBN={978-3-13-139371-5}, publisher={Thieme}, author={Wolf, Sebastian and Kirchhof, Bernd and Reim, Martin}, year={2006}, language={en} }

 @book{Duker_Waheed_Goldman_2022, address={Philadelphia, PA}, edition={Second edition}, title={Handbook of retinal OCT}, ISBN={978-0-323-75772-0}, abstractNote={“Arguably the most important ancillary test available to ophthalmologists worldwide, optical coherence tomography (OCT) has revolutionized the field, and now includes angiographic evaluations (OCTA) that provide vascular flow data without eye injection. Handbook of Retinal OCT is an easy-to-use, high-yield guide to both OCT and OCTA imaging for practitioners at any stage of their career. Highly templated, concise, and portable, this revised edition helps you master the latest imaging methods used to evaluate retinal disease, uveitis, and optic nerve disorders”--Publisher’s description}, publisher={Elsevier}, year={2022}, language={en} }

 @book{Kawasaki_2013, title={Optical Coherence Tomography}, ISBN={978-953-51-1032-3}, url={http://www.intechopen.com/books/optical-coherence-tomography}, DOI={10.5772/56293}, publisher={InTech}, year={2013}, month=mar, language={en} }

  @article{Kermany_database, title={Large Dataset of Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images}, volume={3}, url={https://data.mendeley.com/datasets/rscbjbr9sj/3}, DOI={10.17632/rscbjbr9sj.3}, abstractNote={Be sure to download the most recent version of this dataset to maintain accuracy. This dataset contains thousands of validated OCT and Chest X-Ray images described and analyzed in “Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning”. The images are split into a training set and a testing set of independent patients. Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL. This repository of images is made available for use in research only. How to cite this data: Kermany D, Goldbaum M, Cai W et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell. 2018; 172(5):1122-1131. doi:10.1016/j.cell.2018.02.010.}, publisher={Mendeley Data}, author={Kermany, Daniel and Zhang, Kang and Goldbaum, Michael}, year={2018}, month=jun, language={en} }
 
 @article{Yoo_2020, title={Data for: Improved accuracy in OCT diagnosis of rare retinal disease using few-shot learning with generative adversarial networks}, volume={2}, url={https://data.mendeley.com/datasets/btv6yrdbmv/2}, DOI={10.17632/btv6yrdbmv.2}, abstractNote={The retinal OCT images ofr rare diseases were extracted by using the Google image and Google dataset search that included English keywords including central serous chorioretinopathy (CSC), macular telangiectasia (MacTel), macular hole (MH), Stargardt disease, retinitis pigmentosa (RP). These rare diseases were selected according to a previous review article about OCT diagnosis. The images possessing rare diseases were manually classified by two board-certified ophthalmologists, and ambiguous images were removed to clarify the image domains. Additional file “Segmentation_manual.zip” offers manually segmentedOCT images for pathologic lesions.}, publisher={Mendeley Data}, author={Yoo, TaeKeun}, year={2020}, month=oct, language={en} }

 @misc{DR_dataset, url={https://kaggle.com/competitions/diabetic-retinopathy-detection}, abstractNote={Identify signs of diabetic retinopathy in eye images}, language={en} }

 @misc{Porwal_2018, title={Indian Diabetic Retinopathy Image Dataset (IDRiD)}, url={https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid}, abstractNote={Access the dataset for images of typical diabetic retinopathy lesions and also normal retinal structures annotated at a pixel level, focused on an Indian population. This dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image.}, publisher={IEEE}, author={Porwal, Prasanna}, year={2018}, month=apr, language={en} }

 @misc{Messidor, title={Messidor}, url={https://www.adcis.net/en/third-party/messidor/}, journal={ADCIS}, author={MAFFRE, Guillaume and PATRY, Gervais and GAUTHIER, Bruno and LAY, Julien and ROGER, Damien and ELIE, Mélanie and FOLTETE, Arthur and DONJON, Hugo}, language={en} }

 @misc{STARE, url={https://cecas.clemson.edu/~ahoover/stare/} }

 @misc{E_ophtha, title={E-ophtha}, url={https://www.adcis.net/en/third-party/e-ophtha/}, journal={ADCIS}, author={MAFFRE, Guillaume and PATRY, Gervais and GAUTHIER, Bruno and LAY, Julien and ROGER, Damien and ELIE, Mélanie and FOLTETE, Arthur and DONJON, Hugo}, language={en} }

 @inproceedings{He_Zhang_Ren_Sun_2016, address={Las Vegas, NV, USA}, title={Deep Residual Learning for Image Recognition}, ISBN={978-1-4673-8851-1}, url={http://ieeexplore.ieee.org/document/7780459/}, DOI={10.1109/CVPR.2016.90}, abstractNote={Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.}, booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, year={2016}, month=jun, pages={770–778}, language={en} }

@article{Gholami_Roy_Parthasarathy_Lakshminarayanan_2020, title={OCTID: Optical coherence tomography image database}, volume={81}, ISSN={00457906}, DOI={10.1016/j.compeleceng.2019.106532}, abstractNote={Optical coherence tomography (OCT) is a non-invasive imaging modality which is widely used in clinical ophthalmology. OCT images are capable of visualizing deep retinal layers which is crucial for early diagnosis of retinal diseases. In this paper, we describe a comprehensive open-access database containing more than 500 highresolution images categorized into different pathological conditions. The image classes include Normal (NO), Macular Hole (MH), Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), and Diabetic Retinopathy (DR). The images were obtained from a raster scan protocol with a 2mm scan length and 512x1024 pixel resolution. We have also included 25 normal OCT images with their corresponding ground truth delineations which can be used for an accurate evaluation of OCT image segmentation. In addition, we have provided a user friendly GUI which can be used by clinicians for manual (and semi-automated) segmentation.}, journal={Computers \& Electrical Engineering}, author={Gholami, Peyman and Roy, Priyanka and Parthasarathy, Mohana Kuppuswamy and Lakshminarayanan, Vasudevan}, year={2020}, month=jan, pages={106532}, language={en} }

@article{Simonyan_Zisserman_2015, title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, url={http://arxiv.org/abs/1409.1556}, abstractNote={In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}, note={arXiv:1409.1556 [cs]}, number={arXiv:1409.1556}, publisher={arXiv}, author={Simonyan, Karen and Zisserman, Andrew}, year={2015}, month=apr, language={en} }
 
@article{Zhu_Park_Isola_Efros_2020, title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}, url={http://arxiv.org/abs/1703.10593}, abstractNote={Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transﬁguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.}, note={arXiv:1703.10593 [cs]}, number={arXiv:1703.10593}, publisher={arXiv}, author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.}, year={2020}, month=aug, language={en} }
 
@article{Selvaraju_Cogswell_Das_Vedantam_Parikh_Batra, title={Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization}, abstractNote={We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), ﬂowing into the ﬁnal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing ﬁne-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classiﬁcation, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classiﬁcation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]1 and video at youtu.be/COjUB9Izk6E.}, author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv}, language={en} }

 @misc{1000Fundus_Pytorch_TransferLearning, url={https://kaggle.com/code/saranga7/1000fundus-pytorch-transferlearning}, abstractNote={Explore and run machine learning code with Kaggle Notebooks | Using data from 1000 Fundus images with 39 categories}, language={en} }
 
  @article{HRF_2013, title={Retinal vessel segmentation by improved matched filtering: evaluation on a new high‐resolution fundus image database}, volume={7}, rights={http://onlinelibrary.wiley.com/termsAndConditions#vor}, ISSN={1751-9667, 1751-9667}, DOI={10.1049/iet-ipr.2012.0455}, number={4}, journal={IET Image Processing}, author={Odstrcilik, Jan and Kolar, Radim and Budai, Attila and Hornegger, Joachim and Jan, Jiri and Gazarek, Jiri and Kubena, Tomas and Cernosek, Pavel and Svoboda, Ondrej and Angelopoulou, Elli}, year={2013}, month=jun, pages={373–383}, language={en} }
 
   @misc{STARE, title={The STARE Project}, url={https://cecas.clemson.edu/~ahoover/stare/}, author={Michael H. Goldbaum, MD} }
  
 @article{Krizhevsky_Sutskever_Hinton_2017, title={ImageNet classification with deep convolutional neural networks}, volume={60}, ISSN={0001-0782, 1557-7317}, DOI={10.1145/3065386}, abstractNote={We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.}, number={6}, journal={Communications of the ACM}, author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.}, year={2017}, month=may, pages={84–90}, language={en} }
 
 
